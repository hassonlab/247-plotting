{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is used to generate brain coordinate files for brain maps.\n",
    "\n",
    "Module 1 generates brain coordinate files for all elecs for single patients.\n",
    "\n",
    "Module 2 aggregates brain coordinate files for all patients and sig elecs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1\n",
    "\n",
    "Module 1 is used to generate brain coordinate files for all electrodes per patient\n",
    "\n",
    "Input files\n",
    "- brain coordinate files __[txt]__\n",
    "- electrode name conversion files __[csv]__\n",
    "- encoding result __[folder path]__\n",
    "\n",
    "Output files\n",
    "- brain coordinate + encoding results for all elecs __[txt]__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_df(sid, cor, emb_key):\n",
    "\n",
    "    if sid == 7170:\n",
    "        sid = 717\n",
    "\n",
    "    # Get brain coordinate file\n",
    "    coordinatefilename = f\"data/plotting/brainplot/{sid}/{sid}_{cor}.txt\"\n",
    "\n",
    "    data = pd.read_csv(coordinatefilename, sep=\" \", header=None)\n",
    "    data = data.set_index(0)\n",
    "    data = data.loc[:, 1:4]\n",
    "    print(f\"\\nFor subject {sid}:\\ntxt has {len(data.index)} electrodes\")\n",
    "\n",
    "    # Get electrode name conversion file\n",
    "    elecfilename = f\"data/plotting/brainplot/{sid}/{sid}_elecs.csv\"\n",
    "    elecs = pd.read_csv(elecfilename)\n",
    "    elecs = elecs.dropna()\n",
    "    elecs = elecs.rename(columns={\"elec2\": 0})\n",
    "    elecs.set_index(0, inplace=True)\n",
    "\n",
    "    df = pd.merge(data, elecs, left_index=True, right_index=True)\n",
    "    print(f\"Now subject has {len(df)} electrodes\")\n",
    "\n",
    "    # Create filler columns\n",
    "    for col in emb_key:\n",
    "        df[col] = -1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, path):\n",
    "    # Read in one electrode encoding correlation results\n",
    "    filename = os.path.join(\"data/encoding/\",path, filename)\n",
    "    if len(glob.glob(filename)) == 1:\n",
    "        filename = glob.glob(filename)[0]\n",
    "    elif len(glob.glob(filename)) == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        AssertionError(\"huh this shouldn't happen\")\n",
    "    elec_data = pd.read_csv(filename, header=None)\n",
    "    return elec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(filename, path):\n",
    "    # get max correlation for one electrode file\n",
    "    elec_data = read_file(filename, path)\n",
    "    if isinstance(elec_data, int):\n",
    "        return -1\n",
    "    return max(elec_data.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area(filename, path, lags, chosen_lags):\n",
    "    # get area under the curve for one electrode file\n",
    "    elec_data = read_file(filename, path)\n",
    "    if isinstance(elec_data, int):\n",
    "        return -1\n",
    "    elec_data = elec_data.loc[:, chosen_lags]\n",
    "    x_vals = [lags[lag] / 1000 for lag in chosen_lags]\n",
    "\n",
    "    return np.trapz(elec_data, x=x_vals, axis=1)  # integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_encoding(df, sid, formats, type=\"max\", lags = [], chosen_lags=[]):\n",
    "    \n",
    "    for format in formats:\n",
    "        # print(f\"getting results for {format} embedding\")\n",
    "        for row, values in df.iterrows():\n",
    "            col_name1 = format + \"_prod\"\n",
    "            col_name2 = format + \"_comp\"\n",
    "            prod_name = f\"{sid}_{values['elec']}_prod.csv\"\n",
    "            comp_name = f\"{sid}_{values['elec']}_comp.csv\"\n",
    "            if type == \"max\":\n",
    "                df.loc[row, col_name1] = get_max(prod_name, formats[format])\n",
    "                df.loc[row, col_name2] = get_max(comp_name, formats[format])\n",
    "            elif type == \"area\":\n",
    "                df.loc[row, col_name1] = get_area(prod_name, formats[format], lags, chosen_lags)\n",
    "                df.loc[row, col_name2] = get_area(comp_name, formats[format], lags, chosen_lags)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area_diff(df, emb_key, mode=\"normalized\"):\n",
    "    for col in emb_key:\n",
    "        if \"incorrect\" in col or \"bot\" in col: # incorrect column\n",
    "            pass\n",
    "        else: # correct column\n",
    "            # get column names\n",
    "            col2 = col.replace(\"correct\", \"incorrect\") # incorrect column\n",
    "            col2 = col2.replace(\"top\", \"bot\") # bot column\n",
    "            diff_col = col.replace(\"correct\",\"\")\n",
    "            diff_col = diff_col.replace(\"top\",\"\")\n",
    "\n",
    "            # normalized area diff\n",
    "            df.loc[df[col] < 0, col] = 0  # turn negative area to 0\n",
    "            df.loc[df[col2] < 0, col2] = 0  # turn negative area to 0\n",
    "            if mode == \"normalized\": # normalized area diff\n",
    "                df.loc[:,diff_col] = (df[col] - df[col2]) / df[[col, col2]].max(axis=1)\n",
    "            elif mode == \"normalized2\": # area diff normalized\n",
    "                df.loc[:,diff_col] = df[col] - df[col2]\n",
    "                abs_max = max(abs(df.loc[:,diff_col].max()),abs(df.loc[:,diff_col].min()))\n",
    "                df.loc[:,diff_col] = df.loc[:,diff_col] / abs_max\n",
    "            elif mode == \"none\":\n",
    "                df.loc[:,diff_col] = df[col] - df[col2]\n",
    "            df.drop([col, col2], axis=1, inplace=True) # drop original columns\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(df, sid, emb_keys, dir, cor, project):\n",
    "    df.loc[:,0] = df.index\n",
    "\n",
    "    for col in emb_keys:\n",
    "        sid_file = os.path.join(dir, f\"{sid}_{cor}_{col}.txt\")\n",
    "        # sids_file = os.path.join(dir, f\"{project}_{cor}_{col}.txt\")\n",
    "        \n",
    "        df_output = df.loc[:, [0, 1, 2, 3, 4, \"elec\", col]]\n",
    "        df_output.dropna(inplace=True)\n",
    "        with open(sid_file, \"w\") as outfile:\n",
    "            df_output.to_string(outfile, index=False, header=False)\n",
    "        # with open(sids_file, \"a\") as outfile:\n",
    "        #     df_output.to_string(outfile, index=False, header=False)\n",
    "            \n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Core Arguments ######\n",
    "PRJ_ID = \"tfs\"\n",
    "\n",
    "SIDS = [625] # for testing / 1 patient\n",
    "SIDS = [625, 676, 7170, 798]\n",
    "\n",
    "KEYS = [\"prod\", \"comp\"]\n",
    "\n",
    "COR_TYPE = \"ind\" # unique brain coordinate + brain map per patient\n",
    "COR_TYPE = \"ave\" # average brain coordinates (for several patients)\n",
    "\n",
    "##### Encoding Results Folder #####\n",
    "FORMATS = []\n",
    "for sid in SIDS:\n",
    "    FORMATS.append(\n",
    "        {\n",
    "    # \"glove-all\" : f\"tfs/20230228-all-embs/kw-tfs-full-{sid}-glove50-lag10k-25-all-aligned/*/\",\n",
    "    # \"rand-all\" : f\"tfs/20230228-all-embs/kw-tfs-full-{sid}-glove50-lag10k-25-all-aligned-rand/*/\",\n",
    "    # \"arb-all\" : f\"tfs/20230228-all-embs/kw-tfs-full-{sid}-glove50-lag10k-25-all-aligned-arb/*/\",\n",
    "    # \"gptn-1-all\" : f\"tfs/20230228-all-embs/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-all-aligned/*/\",\n",
    "    # \"gptn-all\" : f\"tfs/20230228-all-embs/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-all-shift-emb-aligned/*/\",\n",
    "    # \"gptn-1-all-l30\" : f\"tfs/20230228-all-embs/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-all-l30-aligned/*/\",\n",
    "    # \"gptn-all-l30\" : f\"tfs/20230228-all-embs/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-all-l30-shift-emb-aligned/*/\",\n",
    "    # \"glove-correct5\" : f\"tfs/20230227-gpt2-preds/kw-tfs-full-{sid}-glove50-lag10k-25-gpt2-xl-correct5/*/\",\n",
    "    # \"glove-incorrect5\" : f\"tfs/20230227-gpt2-preds/kw-tfs-full-{sid}-glove50-lag10k-25-gpt2-xl-incorrect5/*/\",\n",
    "    # \"glove-correct1\" : f\"tfs/20230227-gpt2-preds/kw-tfs-full-{sid}-glove50-lag10k-25-gpt2-xl-correct1/*/\",\n",
    "    # \"glove-incorrect1\" : f\"tfs/20230227-gpt2-preds/kw-tfs-full-{sid}-glove50-lag10k-25-gpt2-xl-incorrect1/*/\",\n",
    "    # \"glove-top0.3\" : f\"tfs/20230227-gpt2-preds/kw-tfs-full-{sid}-glove50-lag10k-25-gpt2-xl-prob/*/\",\n",
    "    # \"glove-bot0.3\" : f\"tfs/20230227-gpt2-preds/kw-tfs-full-{sid}-glove50-lag10k-25-gpt2-xl-improb/*/\",\n",
    "    # \"gptn-1-correct5\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-correct5/*/\",\n",
    "    # \"gptn-1-incorrect5\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-incorrect5/*/\",\n",
    "    # \"gptn-1-correct1\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-correct1/*/\",\n",
    "    # \"gptn-1-incorrect1\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-incorrect1/*/\",\n",
    "    # \"gptn-1-top0.3\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-prob/*/\",\n",
    "    # \"gptn-1-bot0.3\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-improb/*/\",\n",
    "    # \"gptn-1-correct5-l30\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-l30-correct5/*/\",\n",
    "    # \"gptn-1-incorrect5-l30\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-l30-incorrect5/*/\",\n",
    "    # \"gptn-1-correct1-l30\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-l30-correct1/*/\",\n",
    "    # \"gptn-1-incorrect1-l30\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-l30-incorrect1/*/\",\n",
    "    # \"gptn-1-top0.3-l30\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-l30-prob/*/\",\n",
    "    # \"gptn-1-bot0.3-l30\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-l30-improb/*/\",\n",
    "    # \"gptn-correct5\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-shift-emb-correct5/*/\",\n",
    "    # \"gptn-incorrect5\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-shift-emb-incorrect5/*/\",\n",
    "    # \"gptn-correct1\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-shift-emb-correct1/*/\",\n",
    "    # \"gptn-incorrect1\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-shift-emb-incorrect1/*/\",\n",
    "    # \"gptn-top0.3\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-shift-emb-prob/*/\",\n",
    "    # \"gptn-bot0.3\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-shift-emb-improb/*/\",\n",
    "    # \"gptn-correct5-l30\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-shift-emb-l30-correct5/*/\",\n",
    "    # \"gptn-incorrect5-l30\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-shift-emb-l30-incorrect5/*/\",\n",
    "    # \"gptn-correct1-l30\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-shift-emb-l30-correct1/*/\",\n",
    "    # \"gptn-incorrect1-l30\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-shift-emb-l30-incorrect1/*/\",\n",
    "    # \"gptn-top0.3-l30\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-shift-emb-l30-prob/*/\",\n",
    "    # \"gptn-bot0.3-l30\" : f\"tfs/20230226-gpt2-preds/kw-tfs-full-{sid}-gpt2-xl-lag10k-25-shift-emb-l30-improb/*/\",\n",
    "    # \"whisper-en-last\" : f\"tfs/20230210-whisper-encoder-onset/kw-tfs-full-en-onset-{sid}-whisper-tiny.en-l4-wn1-5/*/\",\n",
    "    # \"whisper-de-last\" : f\"tfs/20230212-whisper-decoder/kw-tfs-full-de-{sid}-whisper-tiny.en-l4/*/\",\n",
    "    # \"whisper-de-best\" : f\"tfs/20230212-whisper-decoder/kw-tfs-full-de-{sid}-whisper-tiny.en-l3/*/\",\n",
    "    # \"whisper-last\" : f\"tfs/20230216-whisper-full/kw-tfs-full-{sid}-whisper-tiny.en-l4/*/\",\n",
    "    # \"whisper-best\" : f\"tfs/20230216-whisper-full/kw-tfs-full-{sid}-whisper-tiny.en-l3/*/\",\n",
    "    # \"whisper-en-4-correct5\" : f\"tfs/20230402-whisper-preds/kw-tfs-full-{sid}-whisper-tiny.en-encoder-lag10k-25-correct5/*/\",\n",
    "    # \"whisper-en-4-incorrect5\" : f\"tfs/20230402-whisper-preds/kw-tfs-full-{sid}-whisper-tiny.en-encoder-lag10k-25-incorrect5/*/\",\n",
    "    # \"whisper-en-4-top0.3\" : f\"tfs/20230402-whisper-preds/kw-tfs-full-{sid}-whisper-tiny.en-encoder-lag10k-25-prob/*/\",\n",
    "    # \"whisper-en-4-bot0.3\" : f\"tfs/20230402-whisper-preds/kw-tfs-full-{sid}-whisper-tiny.en-encoder-lag10k-25-improb/*/\",\n",
    "    # \"whisper-de-3-correct5\" : f\"tfs/20230402-whisper-preds/kw-tfs-full-{sid}-whisper-tiny.en-decoder-lag10k-25-correct5/*/\",\n",
    "    # \"whisper-de-3-incorrect5\" : f\"tfs/20230402-whisper-preds/kw-tfs-full-{sid}-whisper-tiny.en-decoder-lag10k-25-incorrect5/*/\",\n",
    "    # \"whisper-de-3-top0.3\" : f\"tfs/20230402-whisper-preds/kw-tfs-full-{sid}-whisper-tiny.en-decoder-lag10k-25-prob/*/\",\n",
    "    # \"whisper-de-3-bot0.3\" : f\"tfs/20230402-whisper-preds/kw-tfs-full-{sid}-whisper-tiny.en-decoder-lag10k-25-improb/*/\",\n",
    "    \"prod-comp-flip\" : f\"tfs/20231016-whisper-pc-flip-best-lag/kw-tfs-full-{sid}-whisper-tiny.en-encoder-var-win-lag10k-25-all-pc-flip-best-lag-4/*/\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Output directory name\n",
    "OUTPUT_DIR = \"results/cor-tfs-area-diff-after\"\n",
    "OUTPUT_DIR = \"results/cor-tfs-area-before-diff\"\n",
    "OUTPUT_DIR = \"results/cor-tfs-area-before-norm-diff\"\n",
    "OUTPUT_DIR = \"results/cor-tfs-area-before-norm2-diff\"\n",
    "OUTPUT_DIR = \"results/cor-tfs-max\"\n",
    "\n",
    "# AREA lags (used for add_area)\n",
    "LAGS = np.arange(-10000,10025,25)\n",
    "AREA_START = -500\n",
    "AREA_END = -100\n",
    "\n",
    "# AREA_START = 100\n",
    "# AREA_END = 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "##### Max correlation #####\n",
    "if \"cor-tfs-max\" in OUTPUT_DIR:\n",
    "    for sid, format in zip(SIDS, FORMATS):\n",
    "        emb_key = [emb + \"_\" + key for emb in format.keys() for key in KEYS]\n",
    "        df = get_base_df(sid, COR_TYPE, emb_key) # get all electrodes\n",
    "        df = add_encoding(df, sid, format, \"max\") # add on the columns from encoding results\n",
    "        save_file(df, sid, emb_key, OUTPUT_DIR, COR_TYPE, PRJ_ID) # save txt files\n",
    "\n",
    "\n",
    "##### Difference in area under the curve #####\n",
    "elif \"cor-tfs-area\" in OUTPUT_DIR:\n",
    "    chosen_lag_idx = [\n",
    "        idx for idx, element in enumerate(LAGS) if (element >= AREA_START) & (element <= AREA_END)\n",
    "    ] # calculate the correct lag idx\n",
    "\n",
    "    for sid, format in zip(SIDS, FORMATS):\n",
    "        emb_key = [emb + \"_\" + key for emb in format.keys() for key in KEYS]\n",
    "        df = get_base_df(sid, COR_TYPE, emb_key) # get all electrodes\n",
    "        df = add_encoding(df, sid, format, \"area\", LAGS, chosen_lag_idx) # add on columns from encoding results\n",
    "        df = get_area_diff(df, emb_key, \"normalized2\") # get area difference\n",
    "\n",
    "        # save txt files\n",
    "        new_emb_key = [col.replace(\"incorrect\",\"\").replace(\"bot\", \"\") for col in emb_key if \"incorrect\" in col or \"bot\" in col]\n",
    "        save_file(df, sid, new_emb_key, OUTPUT_DIR, COR_TYPE, PRJ_ID) # save txt files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2\n",
    "\n",
    "Input files\n",
    "- brain coordinate + encoding results (output of Module 1) __[txt]__\n",
    "- electrode name conversion files __[csv]__\n",
    "- significant electrode list __[csv]__\n",
    "\n",
    "Output files\n",
    "- brain coordinate + encoding results for sig elecs __[txt]__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results(input_dir, sids, cor_type, emb_name, key, sig_name):\n",
    "\n",
    "    df_all = pd.DataFrame()\n",
    "    for sid in sids:\n",
    "\n",
    "        # load coordinate file\n",
    "        cor_filename = os.path.join(input_dir,f\"{sid}_{cor_type}_{emb_name}_{key}.txt\")\n",
    "        df = pd.read_fwf(cor_filename,header=None)\n",
    "\n",
    "        # load significance file\n",
    "        if sig_name:\n",
    "            sig_filename = os.path.join(\"data/plotting/\",f\"tfs-sig-file-{sid}-{sig_name}-{key}.csv\")\n",
    "            sig_df = pd.read_csv(sig_filename)\n",
    "            df = pd.merge(df, sig_df, how='inner', left_on=5, right_on=\"electrode\")\n",
    "        \n",
    "        # aggregate\n",
    "        df_all = pd.concat([df_all,df])\n",
    "\n",
    "    # save aggregate file\n",
    "    df_all = df_all[df_all[6] >= 0.08]\n",
    "    df_output = df_all.loc[:, [0, 1, 2, 3, 4, 6]]\n",
    "    sig_str = \"_sig\"\n",
    "    if sig_name is None:\n",
    "        sig_str = \"\"\n",
    "    aggre_filename = os.path.join(input_dir,f\"tfs_{cor_type}_{emb_name}_{key}{sig_str}.txt\")\n",
    "    print(aggre_filename)\n",
    "    with open(aggre_filename, \"w\") as outfile:\n",
    "        df_output.to_string(outfile, index=False, header=False)\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = OUTPUT_DIR\n",
    "\n",
    "# whether to use significance list\n",
    "SIG_ELECS = True # only sig elecs\n",
    "SIG_ELECS = False\n",
    "\n",
    "\n",
    "if \"cor-tfs-max\" in INPUT_DIR: # significance dict for max cor\n",
    "    SIG_DICT = {\n",
    "        # \"glove-all\" : \"glove\",\n",
    "        # \"rand-all\" : \"glove\",\n",
    "        # \"arb-all\" : \"glove\",\n",
    "        # \"gptn-1-all\" : \"gpt\",\n",
    "        # \"gptn-all\" : \"gpt\",\n",
    "        # \"gptn-1-all-l30\" : \"gpt\",\n",
    "        # \"gptn-all-l30\" : \"gpt\",\n",
    "        # \"glove-correct1\": \"glove\",\n",
    "        # \"glove-incorrect1\": \"glove\",\n",
    "        # \"glove-pred1\": \"glove\",\n",
    "        # \"glove-correct5\": \"glove\",\n",
    "        # \"glove-incorrect5\": \"glove\",\n",
    "        # \"glove-pred5\": \"glove\",\n",
    "        # \"glove-top0.3\" : \"glove\",\n",
    "        # \"glove-bot0.3\" : \"glove\",\n",
    "        # \"glove-pred0.3\" : \"glove\",\n",
    "        # \"gptn-1-correct5\": \"gpt\",\n",
    "        # \"gptn-1-incorrect5\": \"gpt\",\n",
    "        # \"gptn-1-correct1\": \"gpt\",\n",
    "        # \"gptn-1-incorrect1\": \"gpt\",\n",
    "        # \"gptn-1-top0.3\": \"gpt\",\n",
    "        # \"gptn-1-bot0.3\": \"gpt\",\n",
    "        # \"gptn-correct5\": \"gpt\",\n",
    "        # \"gptn-incorrect5\": \"gpt\"\n",
    "        # \"gptn-correct1\": \"gpt\",\n",
    "        # \"gptn-incorrect1\": \"gpt\",\n",
    "        # \"gptn-top0.3\": \"gpt\",\n",
    "        # \"gptn-bot0.3\": \"gpt\",\n",
    "        \"prod-comp-flip\": \"gpt\",\n",
    "    }\n",
    "elif \"cor-tfs-area\" in INPUT_DIR: # significance dict for area\n",
    "    SIG_DICT = {\n",
    "        # \"glove-5\": \"glove\",\n",
    "        # \"glove-1\": \"glove\",\n",
    "        # \"glove-0.3\": \"glove\",\n",
    "        # \"gptn-1-5\": \"gpt\",\n",
    "        # \"gptn-1-1\": \"gpt\",\n",
    "        # \"gptn-1-0.3\": \"gpt\",\n",
    "        # \"gptn-5\": \"gpt\",\n",
    "        # \"gptn-1\": \"gpt\",\n",
    "        # \"gptn-0.3\": \"gpt\",\n",
    "        # \"gptn-1-5-l30\": \"gpt\",\n",
    "        # \"gptn-1-1-l30\": \"gpt\",\n",
    "        # \"gptn-1-0.3-l30\": \"gpt\",\n",
    "        # \"gptn-5-l30\": \"gpt\",\n",
    "        # \"gptn-1-l30\": \"gpt\",\n",
    "        # \"gptn-0.3-l30\": \"gpt\",\n",
    "        # \"whisper-de-3-0.3\": \"whisper-de-best-0.01\",\n",
    "        # \"whisper-de-3-5\": \"whisper-de-best-0.01\",\n",
    "        # \"whisper-en-4-0.3\": \"whisper-en-last-0.01\",\n",
    "        # \"whisper-en-4-5\": \"whisper-en-last-0.01\",\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/cor-tfs-max/tfs_ave_prod-comp-flip_prod.txt\n",
      "results/cor-tfs-max/tfs_ave_prod-comp-flip_comp.txt\n"
     ]
    }
   ],
   "source": [
    "for emb in SIG_DICT.keys():\n",
    "    for key in KEYS:\n",
    "        if SIG_ELECS:\n",
    "            sig_name = SIG_DICT[emb]\n",
    "        else:\n",
    "            sig_name = None\n",
    "        df = aggregate_results(INPUT_DIR, SIDS, COR_TYPE, emb, key, sig_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "591a7b5432774fb1d6f7d1b369fc696284217cfd64410156a35232e242fa6d56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
